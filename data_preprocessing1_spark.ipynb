{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21adea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import scipy.stats\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f487d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"df_pre\") \\\n",
    "    .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627b7e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blood</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>patient</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>patient</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>healthy</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>healthy</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      blood  value\n",
       "0   patient   0.84\n",
       "1   patient   1.05\n",
       "2   patient   1.20\n",
       "3   patient   1.20\n",
       "4   patient   1.39\n",
       "5   patient   1.53\n",
       "6   patient   1.67\n",
       "7   patient   1.80\n",
       "8   patient   1.87\n",
       "9   patient   2.07\n",
       "10  patient   2.11\n",
       "11  healthy   0.54\n",
       "12  healthy   0.64\n",
       "13  healthy   0.64\n",
       "14  healthy   0.75\n",
       "15  healthy   0.76\n",
       "16  healthy   0.81\n",
       "17  healthy   1.16\n",
       "18  healthy   1.20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:/AllData/onewayanova_test.csv\")\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbbd8f",
   "metadata": {},
   "source": [
    "#### pandas_df转成pyspark.sql.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5016e3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark = spark.createDataFrame(df)\n",
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f497cb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  blood|value|\n",
      "+-------+-----+\n",
      "|patient| 0.84|\n",
      "|patient| 1.05|\n",
      "|patient|  1.2|\n",
      "|patient|  1.2|\n",
      "|patient| 1.39|\n",
      "|patient| 1.53|\n",
      "|patient| 1.67|\n",
      "|patient|  1.8|\n",
      "|patient| 1.87|\n",
      "|patient| 2.07|\n",
      "|patient| 2.11|\n",
      "|healthy| 0.54|\n",
      "|healthy| 0.64|\n",
      "|healthy| 0.64|\n",
      "|healthy| 0.75|\n",
      "|healthy| 0.76|\n",
      "|healthy| 0.81|\n",
      "|healthy| 1.16|\n",
      "|healthy|  1.2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66e14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e5538e",
   "metadata": {},
   "source": [
    "#### pyspark.sql.dataframe转成pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325cb07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = df_spark.toPandas()\n",
    "type(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530194c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blood</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>patient</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>patient</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>patient</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>healthy</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>healthy</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      blood  value\n",
       "0   patient   0.84\n",
       "1   patient   1.05\n",
       "2   patient   1.20\n",
       "3   patient   1.20\n",
       "4   patient   1.39\n",
       "5   patient   1.53\n",
       "6   patient   1.67\n",
       "7   patient   1.80\n",
       "8   patient   1.87\n",
       "9   patient   2.07\n",
       "10  patient   2.11\n",
       "11  healthy   0.54\n",
       "12  healthy   0.64\n",
       "13  healthy   0.64\n",
       "14  healthy   0.75\n",
       "15  healthy   0.76\n",
       "16  healthy   0.81\n",
       "17  healthy   1.16\n",
       "18  healthy   1.20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a005d9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a88fe",
   "metadata": {},
   "source": [
    "pyspark.sql.dataframe 转成 pyspark.pandas：两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e39dfb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.pandas.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df_spark.to_pandas_on_spark()\n",
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a304cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.pandas.frame.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11 = df_spark.pandas_api()\n",
    "type(df11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b733493",
   "metadata": {},
   "source": [
    "pyspark.pandas 转成 pyspark.sql.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5396e81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df1.to_spark()\n",
    "type(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e863d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fdc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89a1ba39",
   "metadata": {},
   "source": [
    "#### 方差分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e143b6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C(blood)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.324328</td>\n",
       "      <td>2.324328</td>\n",
       "      <td>18.039093</td>\n",
       "      <td>0.000543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.190441</td>\n",
       "      <td>0.128849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            df    sum_sq   mean_sq          F    PR(>F)\n",
       "C(blood)   1.0  2.324328  2.324328  18.039093  0.000543\n",
       "Residual  17.0  2.190441  0.128849        NaN       NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ols('value ~ C(blood)', df_pandas).fit()\n",
    "anovaResults = anova_lm(model)\n",
    "anovaResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b71e23",
   "metadata": {},
   "source": [
    "#### pandas_df转成pyspark.sql.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f51411a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(anovaResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b16a8060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "spark_results = ps.from_pandas(anovaResults).to_spark()\n",
    "type(spark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4b0223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+------------------+--------------------+\n",
      "|  df|            sum_sq|           mean_sq|                 F|              PR(>F)|\n",
      "+----+------------------+------------------+------------------+--------------------+\n",
      "| 1.0|2.3243275119617217|2.3243275119617217|18.039093197792972|5.433378373621748E-4|\n",
      "|17.0| 2.190440909090909|0.1288494652406417|              null|                null|\n",
      "+----+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4aa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7aee6cc",
   "metadata": {},
   "source": [
    "#### pyspark.df基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c84346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  blood|value|\n",
      "+-------+-----+\n",
      "|patient| 0.84|\n",
      "|patient| 1.05|\n",
      "|patient|  1.2|\n",
      "|patient|  1.2|\n",
      "|patient| 1.39|\n",
      "|patient| 1.53|\n",
      "|patient| 1.67|\n",
      "|patient|  1.8|\n",
      "|patient| 1.87|\n",
      "|patient| 2.07|\n",
      "|patient| 2.11|\n",
      "|healthy| 0.54|\n",
      "|healthy| 0.64|\n",
      "|healthy| 0.64|\n",
      "|healthy| 0.75|\n",
      "|healthy| 0.76|\n",
      "|healthy| 0.81|\n",
      "|healthy| 1.16|\n",
      "|healthy|  1.2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "df = pd.read_csv(\"D:/AllData/onewayanova_test.csv\")\n",
    "df_spark = ps.from_pandas(df).to_spark()\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250d4201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| 0.84|\n",
      "| 1.05|\n",
      "|  1.2|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select(df_spark.value).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5747980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+\n",
      "|  blood|value|        value_more|\n",
      "+-------+-----+------------------+\n",
      "|patient| 0.84|1.8399999999999999|\n",
      "|patient| 1.05|              2.05|\n",
      "|patient|  1.2|               2.2|\n",
      "+-------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_spark.withColumn('value_more', col('value') + 1).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2dd020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  blood|value|\n",
      "+-------+-----+\n",
      "|healthy| 0.54|\n",
      "|healthy| 0.64|\n",
      "|healthy| 0.64|\n",
      "|healthy| 0.75|\n",
      "|healthy| 0.76|\n",
      "|healthy| 0.81|\n",
      "|healthy| 1.16|\n",
      "|healthy|  1.2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter(\"blood == 'healthy'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "725f5d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  blood|value|\n",
      "+-------+-----+\n",
      "|healthy| 0.54|\n",
      "|healthy| 0.64|\n",
      "|healthy| 0.64|\n",
      "|healthy| 0.75|\n",
      "|healthy| 0.76|\n",
      "|healthy| 0.81|\n",
      "|healthy| 1.16|\n",
      "|healthy|  1.2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_healthy = df_spark.where(\"blood == 'healthy'\")\n",
    "df_spark_healthy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d96a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b579ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9d14a76",
   "metadata": {},
   "source": [
    "Scalar Pandas UDF用于向量化标量操作。常常与select和withColumn等函数一起使用。\n",
    "\n",
    "其中调用的Python函数需要使用pandas.Series作为输入并返回一个具有相同长度的pandas.Series。\n",
    "\n",
    "Scalar UDF定义了一个转换，函数输入一个或多个pd.Series,输出一个pd.Series，函数的输出和输入有相同的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c8901fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+\n",
      "|  blood|value|           value+1|\n",
      "+-------+-----+------------------+\n",
      "|healthy| 0.54|              1.54|\n",
      "|healthy| 0.64|1.6400000000000001|\n",
      "|healthy| 0.64|1.6400000000000001|\n",
      "|healthy| 0.75|              1.75|\n",
      "|healthy| 0.76|              1.76|\n",
      "|healthy| 0.81|              1.81|\n",
      "|healthy| 1.16|              2.16|\n",
      "|healthy|  1.2|               2.2|\n",
      "+-------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)     # long是长整型，double是浮点型\n",
    "def pandas_plus_value(series):\n",
    "    return series + 1\n",
    "\n",
    "df_spark_healthy.withColumn(\"value+1\", pandas_plus_value(df_spark_healthy.value)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b181d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+\n",
      "|  blood|value|    value_multiply|\n",
      "+-------+-----+------------------+\n",
      "|healthy| 0.54|            0.2916|\n",
      "|healthy| 0.64|            0.4096|\n",
      "|healthy| 0.64|            0.4096|\n",
      "|healthy| 0.75|            0.5625|\n",
      "|healthy| 0.76|            0.5776|\n",
      "|healthy| 0.81|0.6561000000000001|\n",
      "|healthy| 1.16|            1.3456|\n",
      "|healthy|  1.2|              1.44|\n",
      "+-------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    " \n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "df_spark_healthy.withColumn(\"value_multiply\", multiply_func(df_spark_healthy.value, df_spark_healthy.value)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa9a7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+\n",
      "|  blood|value|    value_multiply|\n",
      "+-------+-----+------------------+\n",
      "|healthy| 0.54|            0.2916|\n",
      "|healthy| 0.64|            0.4096|\n",
      "|healthy| 0.64|            0.4096|\n",
      "|healthy| 0.75|            0.5625|\n",
      "|healthy| 0.76|            0.5776|\n",
      "|healthy| 0.81|0.6561000000000001|\n",
      "|healthy| 1.16|            1.3456|\n",
      "|healthy|  1.2|              1.44|\n",
      "+-------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "@pandas_udf(DoubleType(), PandasUDFType.SCALAR)   # 相当于'double'\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "df_spark_healthy.withColumn(\"value_multiply\", multiply_func(df_spark_healthy.value, df_spark_healthy.value)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31a36984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  blood|value|\n",
      "+-------+-----+\n",
      "|healthy| 0.54|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pandas_filter_func(iterator):\n",
    "    for pandas_df in iterator:\n",
    "        yield pandas_df[pandas_df.value == 0.54]\n",
    "        \n",
    "df_spark_healthy.mapInPandas(pandas_filter_func, schema=df_spark_healthy.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b508b",
   "metadata": {},
   "source": [
    "Grouped map（分组映射）panda_udf与groupBy().apply()一起使用，后者实现了“split-apply-combine”模式\n",
    "\n",
    "定义每个分组的Python计算函数，这里可以使用pandas包或者Python自带方法。\n",
    "\n",
    "一个StructType对象或字符串，它定义输出DataFrame的格式，包括输出特征以及特征类型。\n",
    "\n",
    "需要注意的是，StructType对象中的Dataframe特征顺序需要与分组中的Python计算函数返回特征顺序保持一致。\n",
    "\n",
    "GROUPED_MAP UDF定义了转换：pd.DataFrame -> pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c01473a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|  blood|               value|\n",
      "+-------+--------------------+\n",
      "|healthy|-0.27249999999999996|\n",
      "|healthy|             -0.1725|\n",
      "|healthy|             -0.1725|\n",
      "|healthy|             -0.0625|\n",
      "|healthy|-0.05249999999999999|\n",
      "|healthy|-0.00249999999999...|\n",
      "|healthy|  0.3474999999999999|\n",
      "|healthy| 0.38749999999999996|\n",
      "|patient|  -0.680909090909091|\n",
      "|patient|-0.47090909090909094|\n",
      "|patient|-0.32090909090909103|\n",
      "|patient|-0.32090909090909103|\n",
      "|patient|-0.13090909090909109|\n",
      "|patient|0.009090909090909038|\n",
      "|patient| 0.14909090909090894|\n",
      "|patient| 0.27909090909090906|\n",
      "|patient|  0.3490909090909091|\n",
      "|patient|  0.5490909090909089|\n",
      "|patient|  0.5890909090909089|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(df_spark.schema, PandasUDFType.GROUPED_MAP)    # 'blood string, value double' = df_spark.schema\n",
    "def subtract_mean(df):\n",
    "    value = df.value\n",
    "    return df.assign(value = value - value.mean())\n",
    "    \n",
    "df_spark.groupBy('blood').apply(subtract_mean).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb3a6d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------------+\n",
      "|  blood|value|                col1|\n",
      "+-------+-----+--------------------+\n",
      "|healthy| 0.54|-0.27249999999999996|\n",
      "|healthy| 0.64|             -0.1725|\n",
      "|healthy| 0.64|             -0.1725|\n",
      "|healthy| 0.75|             -0.0625|\n",
      "|healthy| 0.76|-0.05249999999999999|\n",
      "|healthy| 0.81|-0.00249999999999...|\n",
      "|healthy| 1.16|  0.3474999999999999|\n",
      "|healthy|  1.2| 0.38749999999999996|\n",
      "|patient| 0.84|  -0.680909090909091|\n",
      "|patient| 1.05|-0.47090909090909094|\n",
      "|patient|  1.2|-0.32090909090909103|\n",
      "|patient|  1.2|-0.32090909090909103|\n",
      "|patient| 1.39|-0.13090909090909109|\n",
      "|patient| 1.53|0.009090909090909038|\n",
      "|patient| 1.67| 0.14909090909090894|\n",
      "|patient|  1.8| 0.27909090909090906|\n",
      "|patient| 1.87|  0.3490909090909091|\n",
      "|patient| 2.07|  0.5490909090909089|\n",
      "|patient| 2.11|  0.5890909090909089|\n",
      "+-------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('blood string, value double, col1 double', PandasUDFType.GROUPED_MAP)  # 不能用df_spark.schema，因为增加了新的一列col1\n",
    "def subtract_mean(df):\n",
    "    value = df.value\n",
    "    return df.assign(col1 = value - value.mean())\n",
    "    \n",
    "df_spark.groupBy('blood').apply(subtract_mean).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8421c",
   "metadata": {},
   "source": [
    "GROUPED_AGG定义了一个或多个pandas.Series -> 一个scalar，scalar的返回值类型（returnType）应该是原始数据类型\n",
    "\n",
    "Grouped aggregate Panda UDF常常与groupBy().agg()和pyspark.sql.window一起使用。 \n",
    "\n",
    "需要注意的是，这种类型的UDF不支持部分聚合，组或窗口的所有数据都将加载到内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d4e42ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+\n",
      "|  blood|        avg_value|            avg_std|\n",
      "+-------+-----------------+-------------------+\n",
      "|healthy|           0.8125|0.24241346025805932|\n",
      "|patient|1.520909090909091| 0.4217927108297284|\n",
      "+-------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.GROUPED_AGG)\n",
    "def count_num(v):\n",
    "    return v.mean()\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.GROUPED_AGG)\n",
    "def count_std(v):\n",
    "    return v.std()\n",
    "\n",
    "df_spark.groupBy(\"blood\").agg(  count_num(df_spark['value']).alias('avg_value'),  \n",
    "                                count_std(df_spark['value']).alias('avg_std')     ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c6159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7619b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18dc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1278a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
